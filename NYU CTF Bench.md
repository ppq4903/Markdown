# NYU CTF Bench：大型语言模型（LLMs）在攻击性安全评估中的基准数据集与框架解读

用于评估LLMs在解决CTF挑战中的表现

[TOC]



## 一、研究背景与动机

### 1. CTF 竞赛的重要性

CTF（Capture the Flag，夺旗赛）自 1993 年 DEFCON 首次举办以来，已成为网络安全训练的核心工具，可模拟真实安全场景，涵盖**密码学（crypto）、取证（forensics）、二进制漏洞利用（pwn）、逆向工程（rev）、Web 漏洞利用（web）、杂项（misc）** 六大领域，主要分为 “Jeopardy”（挖掘隐藏 flag）和 “Attack-Defense”（攻防对抗）两类。

### 2. LLMs 在网络安全领域的潜力与不足

- **潜力**：LLMs 在代码生成、漏洞检测与修复等软件工程任务中已展现能力，初步研究显示其在解决 CTF 问题上存在可能性。
- **不足**：现有研究范围有限（多依赖人工辅助），缺乏专门用于评估 LLMs 解决 CTF 任务的基准数据集与自动化框架，无法全面验证其在攻击性安全中的自主能力。

### 3. 核心动机

填补 LLMs 在 CTF 任务评估中的空白，构建**可扩展、开源的基准数据集与自动化框架**，模拟 DARPA 网络挑战赛（CGC）的自主场景，评估 LLMs 在多步骤推理、工具调用、真实环境交互中的能力，为 AI 驱动的网络安全解决方案提供研究基础。

## 二、核心贡献

研究的核心贡献分为三大模块，具体如下表所示：

| 贡献类别         | 具体内容                                                     | 关键价值                                                     |
| ---------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 开源基准数据集   | 1. 源自 NYU 年度 CSAW CTF 竞赛（2017-2023 年），共 200 个验证通过的挑战，覆盖 6 大安全领域<br />2. 每个挑战包含元数据（名称、描述、所需文件、端口信息）、Docker 部署配置、flag 真值（模型不可见）<br />3. 难度按 “积分” 划分（1-500 分），资格赛难度较低，决赛难度显著更高 | 解决现有数据集规模小、场景单一问题，提供标准化的 CTF 评估场景 |
| 自动化评估框架   | 1. 支持 5 种 LLMs（3 种黑盒模型：GPT-3.5/4、Claude 3；2 种开源模型：Mixtral、LLaMA 3）<br />2. 集成 8 类网络安全工具（如 Ghidra 反编译、netcat 网络连接、SageMath 密码计算等）<br />3. 实现全流程自动化：挑战加载（Docker 容器 / 本地文件）、工具调用、结果验证、日志记录 | 无需人工干预，可重复验证 LLMs 的自主解决能力                 |
| 专用工具集与流程 | 1. 5 个核心模块：后端（对接 OpenAI/Anthropic API 及开源模型服务）、数据加载（Docker 部署 + 临时文件管理）、外部工具（命令执行、文件创建、反编译等）、日志系统（Markdown/JSON 双格式记录）、提示工程（标准化任务描述）<br />2. 针对不同 CTF 类别适配工具（如 pwn/rev 类启用反编译，web 类禁用避免干扰） | 提升 LLMs 工具调用准确性，降低任务无关操作干扰               |

## 三、NYU CTF Bench 基准数据集详情

### 1. 数据集构成

- **来源**：从 CSAW CTF 竞赛的 568 个原始挑战中筛选验证 200 个，确保软件版本兼容、部署可用（Docker 容器可内外网连接，本地挑战配置完整）。
- **类别分布**：按安全领域划分，具体数量如下表（数据源自 Table 2）：

| 类别                  | 资格赛挑战数 | 决赛挑战数 | 总计 |
| --------------------- | ------------ | ---------- | ---- |
| 密码学（crypto）      | 32           | 20         | 52   |
| 取证（forensics）     | 7            | 8          | 15   |
| 二进制漏洞利用（pwn） | 29           | 10         | 39   |
| 逆向工程（rev）       | 31           | 20         | 51   |
| Web 漏洞利用（web）   | 13           | 6          | 19   |
| 杂项（misc）          | 13           | 11         | 24   |

### 2. 数据结构

采用标准化目录组织：`Year/Competition/Event/Category/Challenge Name`，每个挑战包含两类核心文件：

- **challenge.json**：模型可见（名称、描述、所需文件、端口）+ 模型不可见（flag 真值）；
- **docker-compose.yml**（如需服务器部署）：从 Docker Hub 拉取镜像，构建独立运行环境；
- **辅助文件**：源代码、配置文件、多媒体素材（仅 “files” 字段中列出的文件对模型可见，模拟真实 CTF 场景）。

## 四、自动化评估框架与实验设计

### 1. 框架架构（图 3 核心模块）

| 模块         | 功能描述                                                     |
| ------------ | ------------------------------------------------------------ |
| 后端模块     | 支持 3 类服务：OpenAI（GPT-3.5/4）、Anthropic（Claude 3 Haiku/Sonnet/Opus）、开源模型（TGI/vLLM 部署） |
| 数据加载模块 | 1. Docker 类挑战：通过 docker-compose 拉取镜像、启动容器，赛后自动清理资源2. 本地类挑战：加载源码至临时文件夹，赛后删除 |
| 外部工具模块 | 5 种核心工具：`run_command`（执行 Linux 命令）、`createfile`（创建文件）、`disassemble/decompile`（反编译）、`check_flag`（验证结果）、`give_up`（终止任务） |
| 日志系统     | 实时输出（系统提示、用户指令、模型响应）+ 归档存储（JSON/HTML 格式，含网络 / 挑战 / 模型元数据） |
| 提示模块     | 标准化模板：系统提示（CTF 目标与 flag 格式）+ 用户提示（挑战类别、描述、文件）+ 模型提示（工具调用引导） |

### 2. 实验设计

- **环境配置**：本地服务器部署数据集、Docker 镜像，OpenAI/Anthropic 账号配置 API 密钥，开源模型部署于 Nvidia A100 GPU（TGI/vLLM 框架）；
- **评估规则**：每个挑战重复 5 次，至少 1 次成功视为 “解决”；模型限时 48 小时（模拟真实 CTF 竞赛时长）；
- **失败类型**：分为 5 类 —— 放弃任务、超回合数、连接失败、超 token 限制、答案错误。

## 五、实验结果与关键发现

### 1. LLMs 性能对比（Table 4）

| LLM 模型        | 各类别解决率（%）                            | 总体表现               | 主要失败类型                           |
| --------------- | -------------------------------------------- | ---------------------- | -------------------------------------- |
| GPT-3.5         | crypto:1.92, pwn:2.56, rev:5.88, misc:12.5   | 部分简单任务有效       | 放弃任务（47.15%）、超 token（24.56%） |
| GPT-4           | forensics:6.67, pwn:7.69, rev:9.80, web:5.26 | 总体最佳，但成功率有限 | 放弃任务（38.25%）、答案错误（24.88%） |
| Claude 3        | crypto:5.77, pwn:2.56, rev:3.92, misc:8.33   | 密码学任务表现突出     | 放弃任务（52.99%）、超回合数（42.73%） |
| Mixtral/LLaMA 3 | 全类别 0%                                    | 完全无法解决 CTF 任务  | 答案错误（100%）                       |

### 2. 与人类性能对比（Table 5）

- **人类基准**：2022-2023 年 CSAW CTF 参赛团队（资格赛 1176 队 / 决赛 51 队），平均得分 587-1773 分；

- LLMs 表现

  ：

  - GPT-4 在 2023 资格赛得 300 分（低于人类平均），2022 赛事未得分；
  - GPT-3.5 在 2022 资格赛 / 决赛分别得 500/1000 分（接近人类中位数），2023 赛事未得分；
  - Claude 3 在 2022 决赛得 1500 分（超过人类中位数），2023 赛事未得分；

- **差距原因**：任务复杂度高、决赛难度陡增、模型随机性（默认温度参数）影响稳定性。

### 3. 典型案例分析

- **pwn 挑战（Puffin）**：GPT-4 通过反编译识别缓冲区溢出漏洞，构造 48 字节填充 + 非零值覆盖栈变量，成功执行`cat flag.txt`；
- **misc 挑战（AndroidDropper）**：Claude 3 自动安装 JDK，提取.apk 中的.dex 文件，调用`getFlag()`方法获取 flag；
- **web 挑战（Smug Dino）**：GPT-4 通过分析 HTTP 头与重定向，推测 NGINX 走私漏洞（CVE-2019-20372），47% 案例成功获取 flag（GPT-3.5 成功率 0%）。

## 六、伦理考量与未来工作

### 1. 伦理风险

- **双重用途风险**：LLMs 解决 CTF 的能力可能被滥用为恶意攻击工具（如社会工程、恶意软件生成）；
- **当前现状**：实验中 LLMs 未因伦理冲突拒绝解决 CTF 任务，需进一步优化模型对齐（Alignment），确保其仅用于教育与防御场景。

### 2. 未来改进方向

1. **数据集优化**：平衡类别分布（补充 forensics/web 类挑战），纳入更多赛事数据（如 DEFCON、picoCTF），新增事件响应（IR）类挑战；
2. **工具与平台升级**：优化工具适配逻辑（避免用反编译工具处理 Python 脚本），支持更多开源模型与新型安全工具；
3. **模型能力提升**：结合强化学习（以 “获取 flag” 为奖励信号），改进提示工程，增强 LLMs 的多步骤推理与异常处理能力。

## 七、关键资源与开源链接

- **NYU CTF Bench 数据集**：https://github.com/NYU-LLM-CTF/NYU_CTF_Bench
- **自动化框架（llm_ctf_automation）**：https://github.com/NYU-LLM-CTF/llm_ctf_automation
- **数据集来源（CSAW CTF 挑战库）**：https://github.com/orgs/osirislab/repositories?q=CSAW-CTF